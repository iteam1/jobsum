{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/gom/Workspace/jobsum/research\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Nov 19 13:06:05 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 525.147.05   Driver Version: 525.147.05   CUDA Version: 12.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ...  Off  | 00000000:01:00.0  On |                  N/A |\n",
      "| 40%   49C    P8     1W /  38W |    430MiB /  2048MiB |     48%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA GeForce ...  Off  | 00000000:05:00.0 Off |                  N/A |\n",
      "| 54%   46C    P8    14W / 170W |      6MiB / 12288MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A      1655      G   /usr/lib/xorg/Xorg                108MiB |\n",
      "|    0   N/A  N/A      1878      G   /usr/bin/gnome-shell               43MiB |\n",
      "|    0   N/A  N/A      3189      G   ...8/usr/lib/firefox/firefox      123MiB |\n",
      "|    0   N/A  N/A      3973      G   ...RendererForSitePerProcess      147MiB |\n",
      "|    1   N/A  N/A      1655      G   /usr/lib/xorg/Xorg                  4MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --upgrade accelerate\n",
    "# !pip uninstall -y transformers accelerate\n",
    "# !pip install transformers accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Import Successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/gom/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']=\"0\"\n",
    "\n",
    "import nltk\n",
    "import torch\n",
    "import random\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from datasets import load_dataset, load_from_disk, load_metric\n",
    "from transformers import pipeline, set_seed\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "nltk.download('punkt')\n",
    "print('Import Successfully!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at google/pegasus-cnn_dailymail and are newly initialized: ['model.encoder.embed_positions.weight', 'model.decoder.embed_positions.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_ckpt = \"google/pegasus-cnn_dailymail\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
    "model_pegasus = AutoModelForSeq2SeqLM.from_pretrained(model_ckpt).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dowload & unzip data\n",
    "# !wget https://github.com/entbappy/Branching-tutorial/raw/master/summarizer-data.zip\n",
    "# !unzip summarizer-data.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'dialogue', 'summary'],\n",
       "        num_rows: 14732\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'dialogue', 'summary'],\n",
       "        num_rows: 819\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'dialogue', 'summary'],\n",
       "        num_rows: 818\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_samsum = load_from_disk('samsum_dataset')\n",
    "dataset_samsum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split lenghts: [14732, 819, 818]\n",
      "Features: ['id', 'dialogue', 'summary']\n",
      "\n",
      "Dialogue\n",
      "Eric: MACHINE!\n",
      "Rob: That's so gr8!\n",
      "Eric: I know! And shows how Americans see Russian ;)\n",
      "Rob: And it's really funny!\n",
      "Eric: I know! I especially like the train part!\n",
      "Rob: Hahaha! No one talks to the machine like that!\n",
      "Eric: Is this his only stand-up?\n",
      "Rob: Idk. I'll check.\n",
      "Eric: Sure.\n",
      "Rob: Turns out no! There are some of his stand-ups on youtube.\n",
      "Eric: Gr8! I'll watch them now!\n",
      "Rob: Me too!\n",
      "Eric: MACHINE!\n",
      "Rob: MACHINE!\n",
      "Eric: TTYL?\n",
      "Rob: Sure :)\n",
      "\n",
      "Summary:\n",
      "Eric and Rob are going to watch a stand-up on youtube.\n"
     ]
    }
   ],
   "source": [
    "split_lenghts = [len(dataset_samsum[split]) for split in dataset_samsum]\n",
    "\n",
    "print(f\"Split lenghts: {split_lenghts}\")\n",
    "print(f\"Features: {dataset_samsum['train'].column_names}\")\n",
    "\n",
    "print(\"\\nDialogue\")\n",
    "print(dataset_samsum[\"test\"][1][\"dialogue\"])\n",
    "\n",
    "print(\"\\nSummary:\")\n",
    "print(dataset_samsum[\"test\"][1][\"summary\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_examples_to_features(example_batch):\n",
    "    input_encodings = tokenizer(example_batch['dialogue'],\n",
    "                                max_length=128, #1024\n",
    "                                truncation = True )\n",
    "    \n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        target_encodings = tokenizer(\n",
    "            example_batch['summary'],\n",
    "            max_length=64, # 128\n",
    "            truncation=True\n",
    "        )\n",
    "\n",
    "    return{\n",
    "        'input_ids': input_encodings['input_ids'],\n",
    "        'attention_mask': input_encodings['attention_mask'],\n",
    "        'labels': target_encodings['input_ids']\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/818 [00:00<?, ? examples/s]/home/gom/miniconda3/envs/env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:3856: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 818/818 [00:00<00:00, 14882.13 examples/s]\n"
     ]
    }
   ],
   "source": [
    "dataset_samsum_pt = dataset_samsum.map(convert_examples_to_features, batched = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'dialogue', 'summary', 'input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 14732\n",
       "})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_samsum_pt['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "seq2seq_data_collector = DataCollatorForSeq2Seq(tokenizer, model = model_pegasus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "trainer_args = TrainingArguments(\n",
    "    output_dir='pegasus-samsum', num_train_epochs=10, warmup_steps=500,\n",
    "    per_device_train_batch_size=1, per_gpu_train_batch_size=1,\n",
    "    weight_decay=0.01, logging_steps=10,\n",
    "    evaluation_strategy='steps', eval_steps=500, save_steps=1e6,\n",
    "    gradient_accumulation_steps=16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(model=model_pegasus,\n",
    "                  args=trainer_args,\n",
    "                  tokenizer=tokenizer,\n",
    "                  data_collator=seq2seq_data_collector,\n",
    "                  train_dataset=dataset_samsum_pt['test'],\n",
    "                  eval_dataset=dataset_samsum_pt['validation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
      "  0%|          | 0/510 [00:00<?, ?it/s]You're using a PegasusTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "  2%|â–         | 10/510 [00:16<13:51,  1.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.3339, 'learning_rate': 1.0000000000000002e-06, 'epoch': 0.2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|â–         | 20/510 [00:33<13:40,  1.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.2403, 'learning_rate': 2.0000000000000003e-06, 'epoch': 0.39}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|â–Œ         | 30/510 [00:50<13:33,  1.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.9312, 'learning_rate': 3e-06, 'epoch': 0.59}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|â–Š         | 40/510 [01:07<13:25,  1.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.236, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.78}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|â–‰         | 50/510 [01:24<12:56,  1.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.1074, 'learning_rate': 5e-06, 'epoch': 0.98}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|â–ˆâ–        | 60/510 [01:40<12:30,  1.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.9087, 'learning_rate': 6e-06, 'epoch': 1.17}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|â–ˆâ–Ž        | 70/510 [01:57<12:20,  1.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.7629, 'learning_rate': 7.000000000000001e-06, 'epoch': 1.37}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|â–ˆâ–Œ        | 80/510 [02:14<11:54,  1.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.5819, 'learning_rate': 8.000000000000001e-06, 'epoch': 1.56}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|â–ˆâ–Š        | 90/510 [02:31<11:54,  1.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.6166, 'learning_rate': 9e-06, 'epoch': 1.76}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|â–ˆâ–‰        | 100/510 [02:48<11:39,  1.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.5695, 'learning_rate': 1e-05, 'epoch': 1.95}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|â–ˆâ–ˆâ–       | 110/510 [03:05<11:18,  1.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.4363, 'learning_rate': 1.1000000000000001e-05, 'epoch': 2.15}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|â–ˆâ–ˆâ–Ž       | 120/510 [03:22<11:03,  1.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.2699, 'learning_rate': 1.2e-05, 'epoch': 2.34}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|â–ˆâ–ˆâ–Œ       | 130/510 [03:39<10:43,  1.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.4264, 'learning_rate': 1.3000000000000001e-05, 'epoch': 2.54}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|â–ˆâ–ˆâ–‹       | 140/510 [03:56<10:28,  1.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.214, 'learning_rate': 1.4000000000000001e-05, 'epoch': 2.74}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|â–ˆâ–ˆâ–‰       | 150/510 [04:13<10:20,  1.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.2868, 'learning_rate': 1.5e-05, 'epoch': 2.93}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|â–ˆâ–ˆâ–ˆâ–      | 160/510 [04:30<09:51,  1.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.1011, 'learning_rate': 1.6000000000000003e-05, 'epoch': 3.13}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 170/510 [04:47<09:23,  1.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.0639, 'learning_rate': 1.7000000000000003e-05, 'epoch': 3.32}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 180/510 [05:04<09:36,  1.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.0266, 'learning_rate': 1.8e-05, 'epoch': 3.52}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|â–ˆâ–ˆâ–ˆâ–‹      | 190/510 [05:21<08:58,  1.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.1341, 'learning_rate': 1.9e-05, 'epoch': 3.71}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|â–ˆâ–ˆâ–ˆâ–‰      | 200/510 [05:38<08:52,  1.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.0664, 'learning_rate': 2e-05, 'epoch': 3.91}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 210/510 [05:55<08:32,  1.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9825, 'learning_rate': 2.1e-05, 'epoch': 4.1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 220/510 [06:12<08:05,  1.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.8223, 'learning_rate': 2.2000000000000003e-05, 'epoch': 4.3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 230/510 [06:29<08:00,  1.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9761, 'learning_rate': 2.3000000000000003e-05, 'epoch': 4.49}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 240/510 [06:46<07:43,  1.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.0126, 'learning_rate': 2.4e-05, 'epoch': 4.69}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 250/510 [07:03<07:25,  1.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9364, 'learning_rate': 2.5e-05, 'epoch': 4.88}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 260/510 [07:20<07:03,  1.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9311, 'learning_rate': 2.6000000000000002e-05, 'epoch': 5.08}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 270/510 [07:37<06:46,  1.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.8696, 'learning_rate': 2.7000000000000002e-05, 'epoch': 5.27}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 280/510 [07:54<06:26,  1.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.7726, 'learning_rate': 2.8000000000000003e-05, 'epoch': 5.47}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 290/510 [08:11<06:14,  1.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.6925, 'learning_rate': 2.9e-05, 'epoch': 5.67}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 300/510 [08:28<05:53,  1.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.6853, 'learning_rate': 3e-05, 'epoch': 5.86}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 310/510 [08:45<05:36,  1.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.8515, 'learning_rate': 3.1e-05, 'epoch': 6.06}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 320/510 [09:02<05:21,  1.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.7243, 'learning_rate': 3.2000000000000005e-05, 'epoch': 6.25}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 330/510 [09:19<05:06,  1.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.7035, 'learning_rate': 3.3e-05, 'epoch': 6.45}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 340/510 [09:36<04:51,  1.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.6803, 'learning_rate': 3.4000000000000007e-05, 'epoch': 6.64}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 350/510 [09:53<04:30,  1.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.6914, 'learning_rate': 3.5e-05, 'epoch': 6.84}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 360/510 [10:10<04:09,  1.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.7376, 'learning_rate': 3.6e-05, 'epoch': 7.03}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 370/510 [10:27<03:57,  1.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.5837, 'learning_rate': 3.7e-05, 'epoch': 7.23}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 380/510 [10:44<03:44,  1.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.6498, 'learning_rate': 3.8e-05, 'epoch': 7.42}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 390/510 [11:01<03:22,  1.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.5745, 'learning_rate': 3.9000000000000006e-05, 'epoch': 7.62}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 400/510 [11:18<03:05,  1.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.632, 'learning_rate': 4e-05, 'epoch': 7.81}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 410/510 [11:35<02:50,  1.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.5951, 'learning_rate': 4.1e-05, 'epoch': 8.01}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 420/510 [11:52<02:31,  1.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.5472, 'learning_rate': 4.2e-05, 'epoch': 8.21}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 430/510 [12:09<02:17,  1.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.5172, 'learning_rate': 4.3e-05, 'epoch': 8.4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 440/510 [12:26<01:58,  1.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4684, 'learning_rate': 4.4000000000000006e-05, 'epoch': 8.6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 450/510 [12:43<01:44,  1.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.5487, 'learning_rate': 4.5e-05, 'epoch': 8.79}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 460/510 [13:00<01:22,  1.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.5242, 'learning_rate': 4.600000000000001e-05, 'epoch': 8.99}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 470/510 [13:17<01:08,  1.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4397, 'learning_rate': 4.7e-05, 'epoch': 9.18}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 480/510 [13:34<00:50,  1.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4339, 'learning_rate': 4.8e-05, 'epoch': 9.38}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 490/510 [13:51<00:33,  1.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4193, 'learning_rate': 4.9e-05, 'epoch': 9.57}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 500/510 [14:08<00:16,  1.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3729, 'learning_rate': 5e-05, 'epoch': 9.77}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 500/510 [14:23<00:16,  1.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.8526978492736816, 'eval_runtime': 14.5556, 'eval_samples_per_second': 56.198, 'eval_steps_per_second': 7.076, 'epoch': 9.77}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 510/510 [14:40<00:00,  1.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4144, 'learning_rate': 0.0, 'epoch': 9.96}\n",
      "{'train_runtime': 880.2726, 'train_samples_per_second': 9.304, 'train_steps_per_second': 0.579, 'train_loss': 2.0216531828338025, 'epoch': 9.96}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=510, training_loss=2.0216531828338025, metrics={'train_runtime': 880.2726, 'train_samples_per_second': 9.304, 'train_steps_per_second': 0.579, 'train_loss': 2.0216531828338025, 'epoch': 9.96})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "\n",
    "def generate_batch_sized_chunks(list_of_elements, batch_size):\n",
    "    \"\"\"split the dataset into smaller batches that we can process simultaneously\n",
    "    Yield successive batch-sized chunks from list_of_elements.\"\"\"\n",
    "    for i in range(0, len(list_of_elements), batch_size):\n",
    "        yield list_of_elements[i : i + batch_size]\n",
    "\n",
    "\n",
    "\n",
    "def calculate_metric_on_test_ds(dataset, metric, model, tokenizer, \n",
    "                               batch_size=16, device=device, \n",
    "                               column_text=\"article\", \n",
    "                               column_summary=\"highlights\"):\n",
    "    article_batches = list(generate_batch_sized_chunks(dataset[column_text], batch_size))\n",
    "    target_batches = list(generate_batch_sized_chunks(dataset[column_summary], batch_size))\n",
    "\n",
    "    for article_batch, target_batch in tqdm(\n",
    "        zip(article_batches, target_batches), total=len(article_batches)):\n",
    "        \n",
    "        inputs = tokenizer(article_batch, max_length=1024,  truncation=True, \n",
    "                        padding=\"max_length\", return_tensors=\"pt\")\n",
    "        \n",
    "        summaries = model.generate(input_ids=inputs[\"input_ids\"].to(device),\n",
    "                         attention_mask=inputs[\"attention_mask\"].to(device), \n",
    "                         length_penalty=0.8, num_beams=8, max_length=128)\n",
    "        ''' parameter for length penalty ensures that the model does not generate sequences that are too long. '''\n",
    "        \n",
    "        # Finally, we decode the generated texts, \n",
    "        # replace the  token, and add the decoded texts with the references to the metric.\n",
    "        decoded_summaries = [tokenizer.decode(s, skip_special_tokens=True, \n",
    "                                clean_up_tokenization_spaces=True) \n",
    "               for s in summaries]      \n",
    "        \n",
    "        decoded_summaries = [d.replace(\"\", \" \") for d in decoded_summaries]\n",
    "        \n",
    "        \n",
    "        metric.add_batch(predictions=decoded_summaries, references=target_batch)\n",
    "        \n",
    "    #  Finally compute and return the ROUGE scores.\n",
    "    score = metric.compute()\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_34283/2696170338.py:2: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
      "  rouge_metric = load_metric('rouge')\n"
     ]
    }
   ],
   "source": [
    "rouge_names = [\"rouge1\", \"rouge2\", \"rougeL\", \"rougeLsum\"]\n",
    "rouge_metric = load_metric('rouge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kill PID to release GPU memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "# model_pegasus.save_pretrained(\"pegasus-samsum-model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('tokenizer/tokenizer_config.json',\n",
       " 'tokenizer/special_tokens_map.json',\n",
       " 'tokenizer/spiece.model',\n",
       " 'tokenizer/added_tokens.json',\n",
       " 'tokenizer/tokenizer.json')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Save tokenizer\n",
    "tokenizer.save_pretrained(\"tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pretrained model\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"./pegasus-samsum-model/\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:05<00:00,  1.15s/it]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rouge1</th>\n",
       "      <th>rouge2</th>\n",
       "      <th>rougeL</th>\n",
       "      <th>rougeLsum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>pegasus</th>\n",
       "      <td>0.025278</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.025356</td>\n",
       "      <td>0.025265</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           rouge1  rouge2    rougeL  rougeLsum\n",
       "pegasus  0.025278     0.0  0.025356   0.025265"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score = calculate_metric_on_test_ds(\n",
    "    dataset_samsum['test'][0:10], rouge_metric, model, tokenizer, batch_size = 2, column_text = 'dialogue', column_summary= 'summary'\n",
    ")\n",
    "\n",
    "rouge_dict = dict((rn, score[rn].mid.fmeasure ) for rn in rouge_names )\n",
    "\n",
    "pd.DataFrame(rouge_dict, index = [f'pegasus'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 128, but your input_length is only 99. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=49)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dialogue:\n",
      "Richie: Pogba\n",
      "Clay: Pogboom\n",
      "Richie: what a s strike yoh!\n",
      "Clay: was off the seat the moment he chopped the ball back to his right foot\n",
      "Richie: me too dude\n",
      "Clay: hope his form lasts\n",
      "Richie: This season he's more mature\n",
      "Clay: Yeah, Jose has his trust in him\n",
      "Richie: everyone does\n",
      "Clay: yeah, he really deserved to score after his first 60 minutes\n",
      "Richie: reward\n",
      "Clay: yeah man\n",
      "Richie: cool then \n",
      "Clay: cool\n",
      "\n",
      "Reference Summary:\n",
      "Richie and Clay saw a very good football game, with one football player chopping the ball back to his foot, which was particularly exciting. Jose has trust in that player. \n",
      "\n",
      "Model Summary:\n",
      "Richie and Clay are impressed by Pogba's first half performance. They hope his form lasts this season. Jose has his trust in him and he's more mature.\n"
     ]
    }
   ],
   "source": [
    "#Prediction\n",
    "\n",
    "idx = random.choice(range(dataset_samsum['test'].num_rows))\n",
    "\n",
    "gen_kwargs = {\"length_penalty\": 0.8, \"num_beams\":8, \"max_length\": 128}\n",
    "\n",
    "sample_text = dataset_samsum[\"test\"][idx][\"dialogue\"]\n",
    "\n",
    "reference = dataset_samsum[\"test\"][idx][\"summary\"]\n",
    "\n",
    "pipe = pipeline(\"summarization\", model=\"./pegasus-samsum-model/\",tokenizer=tokenizer)\n",
    "\n",
    "## \n",
    "print(\"Dialogue:\")\n",
    "print(sample_text)\n",
    "\n",
    "\n",
    "print(\"\\nReference Summary:\")\n",
    "print(reference)\n",
    "\n",
    "\n",
    "print(\"\\nModel Summary:\")\n",
    "print(pipe(sample_text, **gen_kwargs)[0][\"summary_text\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
